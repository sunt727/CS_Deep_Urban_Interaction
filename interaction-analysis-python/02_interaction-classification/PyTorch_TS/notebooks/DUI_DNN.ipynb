{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUI DNN\n",
    "Deep Urban Interaction - Deep Neural Network  \n",
    "Interaction Classification with OpenCV, OpenPose, and PyTorch  \n",
    "Ryan Yan Zhang <ryanz@mit.edu>  \n",
    "City Science, MIT Media Lab  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "from pprint import pprint\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from video with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made path: {images_save_path}\n",
      "input_file_name = 180305_ML Test Video_v1.mp4\n",
      "total_frames = 900.0\n",
      "180305_ML Test Video_v1_00000.png is saved\n",
      "180305_ML Test Video_v1_00300.png is saved\n",
      "180305_ML Test Video_v1_00600.png is saved\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# input parameters\n",
    "videos_src_path = '../data/viz/01_input_videos'\n",
    "images_save_path = '../data/viz/02_extracted_images'\n",
    "intervel_second = 10.    # extraction intervel in seconds\n",
    "\n",
    "\n",
    "#main code\n",
    "if not os.path.exists(images_save_path):\n",
    "    os.makedirs(images_save_path)\n",
    "    print('made path: {images_save_path}')\n",
    "    \n",
    "# get file list\n",
    "video_file_names = [f for f in os.listdir(videos_src_path) if os.path.isfile(os.path.join(videos_src_path, f)) and f.endswith('.mp4')]\n",
    "\n",
    "for input_file_name in video_file_names:\n",
    "\n",
    "    print('input_file_name = ' + input_file_name)\n",
    "    vc = cv2.VideoCapture(os.path.join(videos_src_path, input_file_name))\n",
    "    fps = int(round(vc.get(cv2.CAP_PROP_FPS)))\n",
    "    intervel_frame = int(round(intervel_second * fps))\n",
    "    total_frames = vc.get(cv2.CAP_PROP_FRAME_COUNT) # maybe inaccurate\n",
    "    print('total_frames = ' + str(total_frames))\n",
    "\n",
    "    if (vc.isOpened() == False): \n",
    "        print('Error opening video stream or file')\n",
    "        exit()\n",
    "    \n",
    "    frame_get = 0\n",
    "    time_second = 0\n",
    "\n",
    "    while frame_get < total_frames:    # loop reading frames, set last frame\n",
    "        rval, frame = vc.read()\n",
    "        if rval == True:\n",
    "            if frame_get % intervel_frame == 0:    # save image every intervel_frame\n",
    "                time_second = int(frame_get/fps)    # int is floor\n",
    "                image_file_name = '{}_{}.png'.format(os.path.splitext(input_file_name)[0], str(frame_get).zfill(5))\n",
    "                cv2.imwrite(os.path.join(images_save_path, image_file_name), frame)    # save as image\n",
    "                #cv2.waitKey(0)\n",
    "                print('{} is saved'.format(image_file_name))\n",
    "            frame_get += 1\n",
    "\n",
    "    vc.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose json from image with OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made path: C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\03_openpose_jsons\n",
      "openpose json time used: 4.700098514556885\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# input parameters\n",
    "input_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\02_extracted_images'\n",
    "output_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\03_openpose_jsons'\n",
    "openpose_path = r'C:\\OpenPose\\openpose-1.2.1-win64-binaries'\n",
    "\n",
    "# main code\n",
    "if not os.path.exists(input_path):\n",
    "    sys.exit('input path doesn\\'t exist!')\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    print(f'made path: {output_path}')\n",
    "\n",
    "start = time.time()\n",
    "cmd = r'cd {} && bin\\OpenPoseDemo.exe --image_dir \"{}\" --write_json \"{}\" --no_display --render_pose 0'.format(openpose_path, input_path, output_path)\n",
    "subprocess.check_output(cmd, shell=True)\n",
    "print(f'openpose json time used: {(time.time() - start)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## People Bounding box from OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made path: C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\04_bounding_box_images\n",
      "made path: C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\05_bounding_box_jsons\n",
      "180305_ML Test Video_v1_00000_tagged.png is saved\n",
      "180305_ML Test Video_v1_00300_tagged.png is saved\n",
      "180305_ML Test Video_v1_00600_tagged.png is saved\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# constant parameters\n",
    "xx = 0\n",
    "yy = 1\n",
    "\n",
    "# input parameters\n",
    "img_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\02_extracted_images'\n",
    "json_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\03_openpose_jsons'\n",
    "img_out_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\04_bounding_box_images'\n",
    "json_label_path = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\05_bounding_box_jsons'\n",
    "fps = 30\n",
    "joint_confident_threshold = 0.1\n",
    "d_expand_fixed = 50.0\n",
    "\n",
    "# main code\n",
    "if not os.path.exists(img_path):\n",
    "    sys.exit('img_path doesn\\'t exist!')\n",
    "if not os.path.exists(json_path):\n",
    "    sys.exit('json_path doesn\\'t exist!')\n",
    "if not os.path.exists(img_out_path):\n",
    "    os.makedirs(img_out_path)\n",
    "    print(f'made path: {img_out_path}')\n",
    "if not os.path.exists(json_label_path):\n",
    "    os.makedirs(json_label_path)\n",
    "    print(f'made path: {json_label_path}')\n",
    "\n",
    "    \n",
    "# get file list. Make sure that the image names' order is corespondent to the json file names' order!!!\n",
    "img_file_names = [f for f in os.listdir(img_path) if os.path.isfile(os.path.join(img_path, f))]\n",
    "json_file_names = [f for f in os.listdir(json_path) if os.path.isfile(os.path.join(json_path, f))]\n",
    "\n",
    "\n",
    "# loop all the image files, json files, and save result\n",
    "for input_image_file_name in os.listdir(img_path):\n",
    "    if (input_image_file_name.endswith(\".png\") or input_image_file_name.endswith(\".jpg\")):\n",
    "        #print(os.path.join(img_path, input_image_file_name))\n",
    "        input_json_file_name = os.path.splitext(input_image_file_name)[0] + \"_keypoints.json\"\n",
    "        #print(os.path.join(json_path, input_json_file_name))\n",
    "        output_image_file_name = os.path.splitext(input_image_file_name)[0] + \"_tagged.png\"\n",
    "        #print(os.path.join(img_out_path, output_image_file_name))\n",
    "        json_label_file_name = input_image_file_name + \".json\"\n",
    "        \n",
    "        # Load an color image\n",
    "        img = cv2.imread(os.path.join(img_path, input_image_file_name))\n",
    "        # read the json file\n",
    "        with open(os.path.join(json_path, input_json_file_name)) as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # get data of effective joints of people in one json file\n",
    "        joint_confident_threshold = 0.1\n",
    "        # loop person in one json\n",
    "        people_list = []\n",
    "        for i in range(len(json_data[\"people\"])):\n",
    "            # loop joints of one person\n",
    "            effective_joint_xy_list = []\n",
    "            for j in range(int(len(json_data[\"people\"][i][\"pose_keypoints\"])/3)):\n",
    "                if json_data[\"people\"][i][\"pose_keypoints\"][j*3+2] > joint_confident_threshold:\n",
    "                    effective_joint_xy_list.append([json_data[\"people\"][i][\"pose_keypoints\"][j*3+xx], json_data[\"people\"][i][\"pose_keypoints\"][j*3+yy]])\n",
    "            people_list.append(effective_joint_xy_list)\n",
    "        \n",
    "        # analysis to extrapolate the bounding box\n",
    "        # simple max min bounding box of all the joints plus expand distance joint 0 to joint 1 for all 4 side\n",
    "        # loop person\n",
    "        box_list = []\n",
    "        for i in range(len(people_list)):\n",
    "            box = [[9999999,9999999],[0,0]] #[upper_left_xy,lower_right_xy]\n",
    "            joint_list = people_list[i]\n",
    "            d_expand = d_expand_fixed\n",
    "\n",
    "            # loop joint xy\n",
    "            for j in range(len(joint_list)):\n",
    "                joint_xy = joint_list[j]\n",
    "                box = [[min(box[0][xx],joint_xy[xx]),min(box[0][yy],joint_xy[yy])],[max(box[1][xx],joint_xy[xx]),max(box[1][yy],joint_xy[yy])]]\n",
    "            # expend the box by distance joint 0 to joint 1 for all 4 side\n",
    "            box = [[box[0][xx]-d_expand,box[0][yy]-d_expand],[box[1][xx]+d_expand,box[1][yy]+d_expand]]\n",
    "            box_list.append(box)\n",
    "        \n",
    "        # draw the boxes\n",
    "        for i in range(len(box_list)):\n",
    "            box = box_list[i]\n",
    "            cv2.rectangle(img,(int(box[0][xx]),int(box[0][yy])),(int(box[1][xx]),int(box[1][yy])),(255,255,255),2)\n",
    "        \n",
    "        # write the image in to file\n",
    "        cv2.imwrite(os.path.join(img_out_path, output_image_file_name), img)\n",
    "        print('{} is saved'.format(output_image_file_name))\n",
    "        \n",
    "        # construct json_label\n",
    "        data = {}\n",
    "        data['filename'] = input_image_file_name\n",
    "        data['file_ext'] = os.path.splitext(input_image_file_name)[1]\n",
    "        data['file_path'] = img_path\n",
    "        height, width, channels = img.shape\n",
    "        data['file_dim'] = [width, height]\n",
    "        data['people'] = []\n",
    "        for i in range(len(box_list)):\n",
    "            box = box_list[i]\n",
    "            pplData = {}\n",
    "            pplData['tl_coord'] = [int(box[0][xx]),int(box[1][yy])]\n",
    "            pplData['br_coord'] = [int(box[1][xx]),int(box[0][yy])]\n",
    "            pplData['dims'] = [int(box[1][xx]-box[0][xx]), int(box[1][yy]-box[0][yy])]\n",
    "            pplData['area'] = int(box[1][xx]-box[0][xx]) * int(box[1][yy]-box[0][yy])\n",
    "            data['people'].append(pplData)\n",
    "        data['count'] = len(box_list)\n",
    "        \n",
    "        with open(os.path.join(json_label_path, json_label_file_name), 'w') as outfile:\n",
    "            json.dump(data, outfile, indent=4)\n",
    "        \n",
    "        #t = t + 1  # run only first t-1 files\n",
    "        continue\n",
    "    else:\n",
    "        #t = t + 1  # run only first t-1 files\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Classification with PyTorch DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets.folder import ImageFolder, default_loader\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_frozen(model_params):\n",
    "    return (p for p in model_params if not p.requires_grad)\n",
    "\n",
    "\n",
    "def all_trainable(model_params):\n",
    "    return all(p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def all_frozen(model_params):\n",
    "    return all(not p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation transforms\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "IMG_SIZE = 224  #224  #defined by NN model input\n",
    "_mean = [0.485, 0.456, 0.406]\n",
    "_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),  #256  #(IMG_SIZE, IMG_SIZE)  # some images are pretty small\n",
    "    #transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(.3, .3, .3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),  #256  #(IMG_SIZE, IMG_SIZE)\n",
    "    #transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set\n",
    "train_ds = ImageFolder(\"../data/raw/DUI/train\", transform=train_trans, loader=default_loader)\n",
    "val_ds = ImageFolder(\"../data/raw/DUI/valid\", transform=train_trans, loader=default_loader)\n",
    "#print(f'len(train_ds): {len(train_ds)}, len(val_ds): {len(val_ds)}')\n",
    "\n",
    "BATCH_SIZE = 128  #2  #256  #512  #32  #220 for resnet152 on Dell Presison 5520 laptop, 400 for resnet18\n",
    "\n",
    "n_classes = 2\n",
    "\n",
    "# DataLoader\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = models.resnet18(pretrained=True)\n",
    "#model = models.resnet50(pretrained=True)\n",
    "#model = models.resnet101(pretrained=True)\n",
    "#model = models.resnet152(pretrained=True)\n",
    "\n",
    "# Transfer learning or whole model training\n",
    "# Opt.1 Transfer learning\n",
    "'''\n",
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "#print(fall_frozen(model.parameters()): {all_frozen(model.parameters())}')\n",
    "\n",
    "model.fc = nn.Linear(512, n_classes)  # according to the model, 512 for resnet18, 2048 for resnet50 & resnet101 & resnet152\n",
    "\n",
    "model = model.to(device)\n",
    "'''\n",
    "\n",
    "# Opt.2 Whole model training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    get_trainable(model.parameters()),\n",
    "    # model.fc.parameters(),\n",
    "    lr=0.001,\n",
    "    # momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop\n",
    "if False:\n",
    "    N_EPOCHS = 10  #1  #2  #10\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        # start epoch\n",
    "        start_time = time.time()\n",
    "        start_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "        print(f'  Start Time: {start_datetime}')\n",
    "\n",
    "        # Train\n",
    "        model.train()  # IMPORTANT\n",
    "\n",
    "        running_loss, correct = 0.0, 0\n",
    "        for X, y in train_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # with torch.set_grad_enabled(True):\n",
    "            y_ = model(X)\n",
    "            loss = criterion(y_, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            print(f\"    batch loss: {loss.item():0.3f}\")\n",
    "            _, y_label_ = torch.max(y_, 1)\n",
    "            correct += (y_label_ == y).sum().item()\n",
    "            running_loss += loss.item() * X.shape[0]\n",
    "\n",
    "        print(f\"  Train Loss: {running_loss / len(train_dl.dataset)}\")\n",
    "        print(f\"  Train Acc:  {correct / len(train_dl.dataset)}\")\n",
    "\n",
    "\n",
    "        # Eval\n",
    "        model.eval()  # IMPORTANT\n",
    "\n",
    "        running_loss, correct = 0.0, 0\n",
    "        with torch.no_grad():  # IMPORTANT\n",
    "            for X, y in val_dl:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                y_ = model(X)\n",
    "\n",
    "                _, y_label_ = torch.max(y_, 1)\n",
    "                correct += (y_label_ == y).sum().item()\n",
    "\n",
    "                loss = criterion(y_, y)\n",
    "                running_loss += loss.item() * X.shape[0]\n",
    "\n",
    "        print(f\"  Valid Loss: {running_loss / len(val_dl.dataset)}\")\n",
    "        print(f\"  Valid Acc:  {correct / len(val_dl.dataset)}\")\n",
    "\n",
    "        # end epoch\n",
    "        end_time = time.time()\n",
    "        end_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        time_elapsed = end_time - start_time\n",
    "        datetime_elapsed = str(datetime.timedelta(seconds = time_elapsed))\n",
    "        print(f'  End Time: {end_datetime}')\n",
    "        print(f'  Time Elapsed: {datetime_elapsed}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made path: C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\06_interaction_images\n",
      "180305_ML Test Video_v1_00000_interaction.png saved\n",
      "180305_ML Test Video_v1_00300_interaction.png saved\n",
      "180305_ML Test Video_v1_00600_interaction.png saved\n"
     ]
    }
   ],
   "source": [
    "# Predict with Trained Model\n",
    "\n",
    "\n",
    "# save the trained model weights\n",
    "model_weights_path = '../data/saved_model_weights/resnet18_whole'\n",
    "\n",
    "# save a trained model weights\n",
    "if False:\n",
    "    torch.save(model.state_dict(), model_weights_path)\n",
    "\n",
    "# load the trained model weights\n",
    "if True:\n",
    "    from torchvision import models\n",
    "    model = models.resnet18(pretrained=True)  # resnet50, 101, 152\n",
    "    model.load_state_dict(torch.load(model_weights_path))\n",
    "    model = model.to(device)\n",
    "\n",
    "    \n",
    "# function of interaction detection on one image and people json\n",
    "def interaction_detection(json_path, image_save_dir):\n",
    "    \n",
    "    # load 1 json data\n",
    "    with open(json_path) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # load 1 image\n",
    "    image_path = os.path.join(json_data['file_path'], json_data['filename'])\n",
    "    img_BGR = cv2.imread(image_path)\n",
    "    height, width, channels = img_BGR.shape\n",
    "    #img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #plt.imshow(img_gray)\n",
    "    img_RGB = cv2.cvtColor(img_BGR, cv2.COLOR_BGR2RGB)\n",
    "    #plt.imshow(img_RGB)\n",
    "    #plt.title('input image')\n",
    "    #plt.show()\n",
    "\n",
    "    # get pairs in 1 image\n",
    "    crop_to_ext = True\n",
    "    pairs_data = []  # pair_data = [pair_img, i, j, int_interaction]\n",
    "    int_interaction = -1\n",
    "    img_result_BGR = img_BGR\n",
    "    # count largar than 1\n",
    "    if json_data['count'] > 1:\n",
    "        num_ppl = len(json_data['people'])\n",
    "        for j in range(num_ppl):\n",
    "            for i in range(num_ppl):\n",
    "                if i > j:\n",
    "                    person_0 = json_data['people'][j]\n",
    "                    person_1 = json_data['people'][i]\n",
    "                    # create a black image\n",
    "                    new_img_RGB = np.zeros((height,width,3), np.uint8)\n",
    "                    # copy pixels of the regions of two persons\n",
    "                    # person_0\n",
    "                    y0 = min(person_0['tl_coord'][yy], person_0['br_coord'][yy])\n",
    "                    y1 = max(person_0['tl_coord'][yy], person_0['br_coord'][yy])\n",
    "                    x0 = min(person_0['tl_coord'][xx], person_0['br_coord'][xx])\n",
    "                    x1 = max(person_0['tl_coord'][xx], person_0['br_coord'][xx])\n",
    "                    if x0 < 0: x0 = 0\n",
    "                    if y0 < 0: y0 = 0\n",
    "                    new_img_RGB[y0:y1, x0:x1] = img_RGB[y0:y1, x0:x1]\n",
    "                    # person_1\n",
    "                    y2 = min(person_1['tl_coord'][yy], person_1['br_coord'][yy])\n",
    "                    y3 = max(person_1['tl_coord'][yy], person_1['br_coord'][yy])\n",
    "                    x2 = min(person_1['tl_coord'][xx], person_1['br_coord'][xx])\n",
    "                    x3 = max(person_1['tl_coord'][xx], person_1['br_coord'][xx])\n",
    "                    if x2 < 0: x2 = 0\n",
    "                    if y2 < 0: y2 = 0\n",
    "                    new_img_RGB[y2:y3, x2:x3] = img_RGB[y2:y3, x2:x3]\n",
    "                    # if crop_to_ext\n",
    "                    if crop_to_ext:\n",
    "                        x4 = min(x0, x2)\n",
    "                        x5 = max(x1, x3)\n",
    "                        y4 = min(y0, y2)\n",
    "                        y5 = max(y1, y3)\n",
    "                        new_img_RGB = new_img_RGB[y4:y5, x4:x5]\n",
    "                    pairs_data.append([new_img_RGB, i, j, int_interaction])\n",
    "                    # save file for DataLoader\n",
    "                    test_image_path = f'{os.path.splitext(image_path)[0]}_{i}_{j}_{os.path.splitext(image_path)[1]}'\n",
    "                    new_img_BGR = cv2.cvtColor(new_img_RGB, cv2.COLOR_RGB2BGR)\n",
    "                    #cv2.imwrite(test_image_path, new_img_BGR)\n",
    "                    #print(f'{test_image_path} saved. ')\n",
    "\n",
    "            # draw people bounding boxes\n",
    "            # for j:\n",
    "            person = json_data['people'][j]\n",
    "            cv2.rectangle(img_result_BGR, (person['tl_coord'][xx], person['tl_coord'][yy]), (person['br_coord'][xx], person['br_coord'][yy]), (0, 0, 255), 2)\n",
    "\n",
    "        # show people detection\n",
    "        img_result_RGB = cv2.cvtColor(img_result_BGR, cv2.COLOR_BGR2RGB)\n",
    "        #plt.imshow(img_result_RGB)\n",
    "        #plt.title('people detection')\n",
    "        #plt.show()\n",
    "\n",
    "    # predict \n",
    "    model.eval()  # IMPORTANT\n",
    "    with torch.no_grad():  # IMPORTANT\n",
    "        # loop pairs\n",
    "        for n in range(len(pairs_data)):\n",
    "            pair_data = pairs_data[n]\n",
    "            pair_img = pair_data[0]\n",
    "            i = pair_data[1]\n",
    "            j = pair_data[2]\n",
    "            # transforms - resize, normalize, toTensor\n",
    "            pair_img = cv2.resize(pair_img,(IMG_SIZE, IMG_SIZE))\n",
    "            #plt.imshow(pair_img)\n",
    "            #plt.title('input pair image')\n",
    "            #plt.show()\n",
    "            pair_img = pair_img / 255.\n",
    "            pair_img = (pair_img - _mean) / _std\n",
    "            pair_img = pair_img.transpose((2, 0, 1))\n",
    "            X = torch.from_numpy(pair_img)\n",
    "            X = X.unsqueeze(0)\n",
    "            X = X.type(torch.FloatTensor)\n",
    "            X = X.to(device)\n",
    "            #print(f'X: {X}')\n",
    "            #print(f'X.shape: {X.shape}')\n",
    "\n",
    "            '''# plot X as image\n",
    "            # this converts it from GPU to CPU and selects first image in a batch\n",
    "            img = X.cpu().numpy()[0]\n",
    "            #convert image back to Height,Width,Channels\n",
    "            img = np.transpose(img, (1,2,0))\n",
    "            img = img * _std + _mean\n",
    "            #show the image\n",
    "            plt.imshow(img)\n",
    "            plt.title(f'i: {i}; j: {j}. ')\n",
    "            plt.show()'''\n",
    "\n",
    "            # predict\n",
    "            y_ = model(X)\n",
    "            _, y_label_ = torch.max(y_, 1)\n",
    "            #print(f'y_label_: \\t\\t{y_label_}')\n",
    "            int_interaction = y_label_.cpu().numpy()[0]\n",
    "            #print(f'int_interaction: {int_interaction}')\n",
    "            pair_data[3] = int_interaction\n",
    "            pairs_data[n] = pair_data\n",
    "\n",
    "            # output result image\n",
    "            if int_interaction == 1:\n",
    "                person_0 = json_data['people'][j]\n",
    "                person_1 = json_data['people'][i]\n",
    "                ppl_center_0 = (int((person_0['tl_coord'][xx] + person_0['br_coord'][xx]) / 2.0), int((person_0['tl_coord'][yy] + person_0['br_coord'][yy]) / 2.0))\n",
    "                ppl_center_1 = (int((person_1['tl_coord'][xx] + person_1['br_coord'][xx]) / 2.0), int((person_1['tl_coord'][yy] + person_1['br_coord'][yy]) / 2.0))\n",
    "                cv2.line(img_result_BGR, ppl_center_0, ppl_center_1, (0,255,0), 5)\n",
    "\n",
    "    #print(f'pairs_data: {pairs_data}')\n",
    "    img_result_RGB = cv2.cvtColor(img_result_BGR, cv2.COLOR_BGR2RGB)\n",
    "    #plt.imshow(img_result_RGB)\n",
    "    #plt.title('interaction detection')\n",
    "    #plt.show()\n",
    "    \n",
    "    # write the image in to file\n",
    "    image_name = json_data['filename']\n",
    "    image_save_name = f'{os.path.splitext(image_name)[0]}_interaction{os.path.splitext(image_name)[1]}'\n",
    "    image_save_path = os.path.join(image_save_dir, image_save_name)\n",
    "    cv2.imwrite(image_save_path, img_result_BGR)\n",
    "    print(f'{image_save_name} saved')\n",
    "\n",
    "\n",
    "    '''# load test data set\n",
    "    #test_ds = ImageFolder(\"../data/raw/DUI/test\", transform=val_trans, loader=default_loader)\n",
    "    test_ds = ImageFolder('C:\\\\Users\\\\RYAN\\\\Documents\\\\GitHub\\\\CS_Urban_Interaction_CV\\\\interaction-analysis-python\\\\02_interaction-classification\\\\PyTorch_RZ\\\\data\\\\viz\\\\06_dnn_test_images', transform=val_trans, loader=default_loader)\n",
    "    #print(f'len(test_ds) = {len(test_ds)}. ')\n",
    "\n",
    "    test_dl = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    #print(f'test_ds[99]: \\n{test_ds[99]}')\n",
    "    #print(f'test_ds[99][1]: \\n{test_ds[99][1]}')\n",
    "\n",
    "\n",
    "    # predict\n",
    "    model.eval()  # IMPORTANT\n",
    "    with torch.no_grad():  # IMPORTANT\n",
    "        for X, y in test_dl:\n",
    "            # plot X as image\n",
    "            # this converts it from GPU to CPU and selects first image in a batch\n",
    "            img = X.cpu().numpy()[0]\n",
    "            #convert image back to Height,Width,Channels\n",
    "            img = np.transpose(img, (1,2,0))\n",
    "            img = img * _std + _mean\n",
    "            #show the image\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "\n",
    "            # predict one batch\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            print(f'X: {X}')\n",
    "            print(f'X.shape: {X.shape}')\n",
    "            print(f'y: \\t\\t\\t{y}')\n",
    "\n",
    "            y_ = model(X)\n",
    "            _, y_label_ = torch.max(y_, 1)\n",
    "            print(f'y_label_: \\t\\t{y_label_}')\n",
    "\n",
    "            is_correct = 'correct' if y_label_ == y else 'wrong'\n",
    "            print(f'is_correct: \\t{is_correct}\\n')'''\n",
    "    \n",
    "    return img_result_RGB\n",
    "\n",
    "\n",
    "# input parameters\n",
    "people_json_dir = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\05_bounding_box_jsons'\n",
    "image_save_dir = r'C:\\Users\\RYAN\\Documents\\GitHub\\CS_Urban_Interaction_CV\\interaction-analysis-python\\02_interaction-classification\\PyTorch_RZ\\data\\viz\\06_interaction_images'\n",
    "\n",
    "# main code\n",
    "if not os.path.exists(people_json_dir):\n",
    "    sys.exit('people_json_dir doesn\\'t exist!')\n",
    "if not os.path.exists(image_save_dir):\n",
    "    os.makedirs(image_save_dir)\n",
    "    print(f'made path: {image_save_dir}')\n",
    "    \n",
    "# get people bounding box json list\n",
    "people_json_filenames = [f for f in os.listdir(people_json_dir) if os.path.isfile(os.path.join(people_json_dir, f)) and f.endswith('.json')]\n",
    "\n",
    "# loop images and people bounding box jsons\n",
    "for people_json_filename in people_json_filenames:\n",
    "    people_json_path = os.path.join(people_json_dir, people_json_filename)\n",
    "    interaction_detection(people_json_path, image_save_dir)\n",
    "\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
